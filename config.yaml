model:
  base_path: gustavecortal/Piaget-0.6B
  tokenizer_trust_remote_code: true
dataset:
  name: Psychotherapy-LLM/PsychoCounsel-Preference
  save_path: psychocounsel_processed
  add_no_think_suffix: r" \no_think"
  max_length: 1024
  num_proc: 8
  limit_train: null
  limit_test: null
training:
  torch_dtype: bfloat16
  device_map: auto
  allow_tf32: true
  lora:
    r: 4
    lora_alpha: 8
    lora_dropout: 0.05
    bias: none
    task_type: CAUSAL_LM
    target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - up_proj
    - gate_proj
    - down_proj
  orpo:
    num_train_epochs: 1
    learning_rate: 0.0005
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 8
    gradient_accumulation_steps: 1
    logging_steps: 50
    eval_steps: 1000
    save_steps: 250
    max_length: 1024
    max_prompt_length: 128
    beta: 0.1
    bf16: false
    remove_unused_columns: false
    gradient_checkpointing: true
    report_to:
    - wandb
    warmup_steps: 500
    eval_strategy: steps
  output_dir: piaget-1.7b-test-a100
  soup:
    combination_types:
    - linear
    - ties
    weights: null
    density: 0.2
