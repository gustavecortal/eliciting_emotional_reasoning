model:
  base_path: gustavecortal/Piaget-0.6B
  tokenizer_trust_remote_code: true
dataset:
  name: Psychotherapy-LLM/PsychoCounsel-Preference
  save_path: psychocounsel_processed
  add_no_think_suffix: r" \no_think"
  max_length: 1024
  num_proc: 8
  limit_train: null
  limit_test: null
training:
  torch_dtype: bfloat16
  device_map: auto
  allow_tf32: true
  lora:
    r: 1
    lora_alpha: 2
    lora_dropout: 0.05
    bias: none
    task_type: CAUSAL_LM
    target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - up_proj
    - gate_proj
    - down_proj
  orpo:
    output_dir: piaget-1.7b-test-a100
    num_train_epochs: 1
    learning_rate: 5e-4
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 1
    gradient_accumulation_steps: 1
    logging_steps: 50
    eval_steps: 1000
    save_steps: 250
    max_length: 1024
    max_prompt_length: 128
    beta: 0.1
    bf16: false
    remove_unused_columns: false
    gradient_checkpointing: true
    report_to:
    - wandb
    warmup_steps: 500
    eval_strategy: steps
  save_model_path: piaget1.7b-test-a100
soup:
  output_dir: null
  combination_types:
  - linear
  - ties
  weights: null
  density: 0.2
  save_dir: merged_models/piaget-1.7b-test-a100
  torch_dtype: bfloat16
  device_map: auto
