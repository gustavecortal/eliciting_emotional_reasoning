model:
  base_path: ../pretrained_models/models--gustavecortal--Piaget-4B/snapshots/13fbcb0b4574fa4daf4c5421d50c6bb76ba8689f
  tokenizer_trust_remote_code: true
dataset:
  name: ../PsychoCounsel-Preference
  save_path: psychocounsel_processed
  add_no_think_suffix: r" \no_think"
  max_length: 1024
  num_proc: 8
  limit_train: null
  limit_test: null
training:
  resume_from_checkpoint: true
  torch_dtype: bfloat16
  device_map: None
  allow_tf32: true
  lora:
    r: 4
    lora_alpha: 8
    lora_dropout: 0.05
    bias: none
    task_type: CAUSAL_LM
    target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - up_proj
    - gate_proj
    - down_proj
  orpo:
    num_train_epochs: 1
    learning_rate: 0.0005
    per_device_train_batch_size: 2
    per_device_eval_batch_size: 2
    gradient_accumulation_steps: 4
    logging_steps: 50
    eval_steps: 2000
    save_steps: 100
    max_length: 1024
    max_prompt_length: 128
    beta: 0.1
    bf16: true
    remove_unused_columns: false
    gradient_checkpointing: false
    report_to:
    - wandb
    warmup_ratio: 0.1
    eval_strategy: steps
  output_dir: roger-4b
  soup:
    combination_types:
    - linear
    - dare_ties
    weights: null
    density: 0.2
